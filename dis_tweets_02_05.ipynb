{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24f902d-29b2-459e-bb9c-d602a05e2bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9caf1-95c3-435f-a5e1-9c276e0c06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "from string import digits\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5594fb79-bd68-46fb-a43d-6533b1e9bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"SRC_projects/nlp_disaster/train.csv\")\n",
    "test = pd.read_csv(\"SRC_projects/nlp_disaster/test.csv\")\n",
    "sample_sub = pd.read_csv(\"SRC_projects/nlp_disaster/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970f399a-434d-4505-9acd-7e9efeb1d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nombre de lignes pour le train set: \",train.shape[0])\n",
    "display(train.head(2))\n",
    "print(\"nombre de lignes pour le test set: \",test.shape[0])\n",
    "display(test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08648a-f64e-4687-93d8-8df04c013993",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pourcentage de valeurs manquantes sur le train set:\")\n",
    "print(train.isna().sum()*100 / train.shape[0])\n",
    "print(\"Pourcentage de valeurs manquantes sur le test set:\")\n",
    "print(test.isna().sum()*100 / test.shape[0]) \n",
    "#On peut déja enlever la colonne location\n",
    "#Je vais faire un dropna pour enlever les valeurs manquantes, la colonne keyword pourra m'être utile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cfecd-b9af-41e5-bca2-93e91bbf1e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opérations sur le train set:\n",
    "\n",
    "train = train.drop('location', axis=1)\n",
    "train = train.dropna()\n",
    "train = train.reset_index()\n",
    "train = train.drop('id',axis=1)\n",
    "train = train.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad90481-5c9f-4873-96ea-bf29d239b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opérations sur le test set:\n",
    "\n",
    "test = test.drop('location', axis=1)\n",
    "test = test.dropna()\n",
    "test = test.reset_index()\n",
    "test = test.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd60d0-186d-4e55-ad47-b51f95ce07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(train['target'].value_counts(),\n",
    "       labels=train['target'].unique(),\n",
    "       autopct=\"%.1f\",\n",
    "        shadow=True,\n",
    "        startangle=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f48848-f11a-4e13-b446-091cb55c05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df): #fonction qui va effectuer le nettoyage et l'encodage de nos datasets\n",
    "    \n",
    "    table = str.maketrans('', '', digits) #va me permettre d'enlever les numéros de la colonne text\n",
    "    \n",
    "    tt = TweetTokenizer() #fonction créee pour analyser les tweets, les convertir en petits tokens\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1000)\n",
    "    \n",
    "    df['text_clean'] = df['text'].apply(lambda x: x.translate(table)).apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)).lower())\n",
    "    # On supprime les ponctuations, les symboles et on enlève toutes les majuscules\n",
    "    \n",
    "    df['text_tokenized'] = df['text_clean'].apply(tt.tokenize)\n",
    "    #On applique le tweet tokenizer sur la colonne de text clean pour obtenir de petits tokens\n",
    "    tokenizer.fit_on_texts(df.text_tokenized)\n",
    "    \n",
    "    test['text_encoded'] = tokenizer.texts_to_sequences(test.text_tokenized) \n",
    "    #On crée une colonne encodée, chaque token est représenté par un chiffre ou nombre \n",
    "    \n",
    "    df['len_text_encoded'] = df['text_encoded'].apply(lambda x: len(x))\n",
    "    #On crée une colonne qui représente la longueur de chaque texte encodé\n",
    "    \n",
    "    df = df[df['len_text_encoded'] != 0] #J'applique un masque pour enlever tous les textes encodés nuls\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d55de31-1317-4f0f-bcc7-4b4b13f4c25c",
   "metadata": {},
   "source": [
    "#Encodage du train set\n",
    "train['text_clean'] = train['text'].apply(lambda x: x.translate(table)).apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)).lower())\n",
    "train.head()\n",
    "#str.maketrans('', '', digits) nous permet d'enlever les numbers\n",
    "#translate(str.maketrans('', '', string.punctuation)).lower() nous permet d'enlever la ponctuation et les majuscules \n",
    "\n",
    "tt = TweetTokenizer()\n",
    "train['text_tokenized'] = train['text_clean'].apply(tt.tokenize) #On onbtient une nouvelle colonne avec le text tokenizé\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(train.text_tokenized)\n",
    "train['text_encoded'] = tokenizer.texts_to_sequences(train.text_tokenized) #Nouvelle colonne, le texte est maintenant encodé\n",
    "\n",
    "train['len_text_encoded'] = train['text_encoded'].apply(lambda x: len(x))\n",
    "train = train[train['len_text_encoded']!=0] #nouvelle colonne avec la longueur de chaque liste de texte encodé\n",
    "\n",
    "#Encodage du test set\n",
    "test['text_clean'] = test['text'].apply(lambda x: x.translate(table)).apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)).lower())\n",
    "test.head()\n",
    "\n",
    "test['text_tokenized'] = test['text_clean'].apply(tt.tokenize)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(test.text_tokenized)\n",
    "test['text_encoded'] = tokenizer.texts_to_sequences(test.text_tokenized)\n",
    "\n",
    "test['len_text_encoded'] = test['text_encoded'].apply(lambda x: len(x))\n",
    "test = test[test['len_text_encoded']!=0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
